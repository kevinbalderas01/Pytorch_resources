{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb03303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59b2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d156a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./shakespeare.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edca10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6b0aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7dc53fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d0cf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f2e094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters = set(text)\n",
    "all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8f26b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32c3653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'a')\n",
      "(1, 'k')\n",
      "(2, '\\n')\n",
      "(3, 't')\n",
      "(4, 'O')\n",
      "(5, '!')\n",
      "(6, '2')\n",
      "(7, 'M')\n",
      "(8, '9')\n",
      "(9, 'i')\n",
      "(10, '-')\n",
      "(11, '5')\n",
      "(12, '4')\n",
      "(13, ',')\n",
      "(14, '_')\n",
      "(15, 'D')\n",
      "(16, 'x')\n",
      "(17, 'Y')\n",
      "(18, '<')\n",
      "(19, 'L')\n",
      "(20, 'J')\n",
      "(21, 'l')\n",
      "(22, 'd')\n",
      "(23, '(')\n",
      "(24, '7')\n",
      "(25, '`')\n",
      "(26, '&')\n",
      "(27, ' ')\n",
      "(28, 'S')\n",
      "(29, 'q')\n",
      "(30, 'Z')\n",
      "(31, ':')\n",
      "(32, 'p')\n",
      "(33, 'X')\n",
      "(34, 'C')\n",
      "(35, '[')\n",
      "(36, 'N')\n",
      "(37, 'm')\n",
      "(38, 'A')\n",
      "(39, '.')\n",
      "(40, 'Q')\n",
      "(41, 'I')\n",
      "(42, 'h')\n",
      "(43, 'T')\n",
      "(44, 'w')\n",
      "(45, 'P')\n",
      "(46, 'e')\n",
      "(47, '0')\n",
      "(48, 'g')\n",
      "(49, '|')\n",
      "(50, 'F')\n",
      "(51, '>')\n",
      "(52, 'n')\n",
      "(53, '8')\n",
      "(54, 'W')\n",
      "(55, '3')\n",
      "(56, 'j')\n",
      "(57, '\"')\n",
      "(58, 'B')\n",
      "(59, 'z')\n",
      "(60, 'b')\n",
      "(61, 'E')\n",
      "(62, '6')\n",
      "(63, 'G')\n",
      "(64, 'y')\n",
      "(65, ']')\n",
      "(66, '1')\n",
      "(67, ')')\n",
      "(68, 'R')\n",
      "(69, \"'\")\n",
      "(70, '}')\n",
      "(71, 'r')\n",
      "(72, ';')\n",
      "(73, 'V')\n",
      "(74, 'c')\n",
      "(75, 'f')\n",
      "(76, 'K')\n",
      "(77, 's')\n",
      "(78, '?')\n",
      "(79, 'o')\n",
      "(80, 'H')\n",
      "(81, 'v')\n",
      "(82, 'u')\n",
      "(83, 'U')\n"
     ]
    }
   ],
   "source": [
    "for pair in enumerate(all_characters):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "839da7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a',\n",
       " 1: 'k',\n",
       " 2: '\\n',\n",
       " 3: 't',\n",
       " 4: 'O',\n",
       " 5: '!',\n",
       " 6: '2',\n",
       " 7: 'M',\n",
       " 8: '9',\n",
       " 9: 'i',\n",
       " 10: '-',\n",
       " 11: '5',\n",
       " 12: '4',\n",
       " 13: ',',\n",
       " 14: '_',\n",
       " 15: 'D',\n",
       " 16: 'x',\n",
       " 17: 'Y',\n",
       " 18: '<',\n",
       " 19: 'L',\n",
       " 20: 'J',\n",
       " 21: 'l',\n",
       " 22: 'd',\n",
       " 23: '(',\n",
       " 24: '7',\n",
       " 25: '`',\n",
       " 26: '&',\n",
       " 27: ' ',\n",
       " 28: 'S',\n",
       " 29: 'q',\n",
       " 30: 'Z',\n",
       " 31: ':',\n",
       " 32: 'p',\n",
       " 33: 'X',\n",
       " 34: 'C',\n",
       " 35: '[',\n",
       " 36: 'N',\n",
       " 37: 'm',\n",
       " 38: 'A',\n",
       " 39: '.',\n",
       " 40: 'Q',\n",
       " 41: 'I',\n",
       " 42: 'h',\n",
       " 43: 'T',\n",
       " 44: 'w',\n",
       " 45: 'P',\n",
       " 46: 'e',\n",
       " 47: '0',\n",
       " 48: 'g',\n",
       " 49: '|',\n",
       " 50: 'F',\n",
       " 51: '>',\n",
       " 52: 'n',\n",
       " 53: '8',\n",
       " 54: 'W',\n",
       " 55: '3',\n",
       " 56: 'j',\n",
       " 57: '\"',\n",
       " 58: 'B',\n",
       " 59: 'z',\n",
       " 60: 'b',\n",
       " 61: 'E',\n",
       " 62: '6',\n",
       " 63: 'G',\n",
       " 64: 'y',\n",
       " 65: ']',\n",
       " 66: '1',\n",
       " 67: ')',\n",
       " 68: 'R',\n",
       " 69: \"'\",\n",
       " 70: '}',\n",
       " 71: 'r',\n",
       " 72: ';',\n",
       " 73: 'V',\n",
       " 74: 'c',\n",
       " 75: 'f',\n",
       " 76: 'K',\n",
       " 77: 's',\n",
       " 78: '?',\n",
       " 79: 'o',\n",
       " 80: 'H',\n",
       " 81: 'v',\n",
       " 82: 'u',\n",
       " 83: 'U'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = dict(enumerate(all_characters))\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e732452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'k': 1,\n",
       " '\\n': 2,\n",
       " 't': 3,\n",
       " 'O': 4,\n",
       " '!': 5,\n",
       " '2': 6,\n",
       " 'M': 7,\n",
       " '9': 8,\n",
       " 'i': 9,\n",
       " '-': 10,\n",
       " '5': 11,\n",
       " '4': 12,\n",
       " ',': 13,\n",
       " '_': 14,\n",
       " 'D': 15,\n",
       " 'x': 16,\n",
       " 'Y': 17,\n",
       " '<': 18,\n",
       " 'L': 19,\n",
       " 'J': 20,\n",
       " 'l': 21,\n",
       " 'd': 22,\n",
       " '(': 23,\n",
       " '7': 24,\n",
       " '`': 25,\n",
       " '&': 26,\n",
       " ' ': 27,\n",
       " 'S': 28,\n",
       " 'q': 29,\n",
       " 'Z': 30,\n",
       " ':': 31,\n",
       " 'p': 32,\n",
       " 'X': 33,\n",
       " 'C': 34,\n",
       " '[': 35,\n",
       " 'N': 36,\n",
       " 'm': 37,\n",
       " 'A': 38,\n",
       " '.': 39,\n",
       " 'Q': 40,\n",
       " 'I': 41,\n",
       " 'h': 42,\n",
       " 'T': 43,\n",
       " 'w': 44,\n",
       " 'P': 45,\n",
       " 'e': 46,\n",
       " '0': 47,\n",
       " 'g': 48,\n",
       " '|': 49,\n",
       " 'F': 50,\n",
       " '>': 51,\n",
       " 'n': 52,\n",
       " '8': 53,\n",
       " 'W': 54,\n",
       " '3': 55,\n",
       " 'j': 56,\n",
       " '\"': 57,\n",
       " 'B': 58,\n",
       " 'z': 59,\n",
       " 'b': 60,\n",
       " 'E': 61,\n",
       " '6': 62,\n",
       " 'G': 63,\n",
       " 'y': 64,\n",
       " ']': 65,\n",
       " '1': 66,\n",
       " ')': 67,\n",
       " 'R': 68,\n",
       " \"'\": 69,\n",
       " '}': 70,\n",
       " 'r': 71,\n",
       " ';': 72,\n",
       " 'V': 73,\n",
       " 'c': 74,\n",
       " 'f': 75,\n",
       " 'K': 76,\n",
       " 's': 77,\n",
       " '?': 78,\n",
       " 'o': 79,\n",
       " 'H': 80,\n",
       " 'v': 81,\n",
       " 'u': 82,\n",
       " 'U': 83}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea4fad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27, 27, 27, 66,  2, 27, 27, 50, 71, 79, 37, 27, 75,  0,  9,\n",
       "       71, 46, 77,  3, 27, 74, 71, 46,  0,  3, 82, 71, 46, 77, 27, 44, 46,\n",
       "       27, 22, 46, 77,  9, 71, 46, 27,  9, 52, 74, 71, 46,  0, 77, 46, 13,\n",
       "        2, 27, 27, 43, 42,  0,  3, 27,  3, 42, 46, 71, 46, 60, 64, 27, 60,\n",
       "       46,  0, 82,  3, 64, 69, 77, 27, 71, 79, 77, 46, 27, 37,  9, 48, 42,\n",
       "        3, 27, 52, 46, 81, 46, 71, 27, 22,  9, 46, 13,  2, 27, 27, 58, 82,\n",
       "        3, 27,  0, 77, 27,  3, 42, 46, 27, 71,  9, 32, 46, 71, 27, 77, 42,\n",
       "       79, 82, 21, 22, 27, 60, 64, 27,  3,  9, 37, 46, 27, 22, 46, 74, 46,\n",
       "        0, 77, 46, 13,  2, 27, 27, 80,  9, 77, 27,  3, 46, 52, 22, 46, 71,\n",
       "       27, 42, 46,  9, 71, 27, 37,  9, 48, 42,  3, 27, 60, 46,  0, 71, 27,\n",
       "       42,  9, 77, 27, 37, 46, 37, 79, 71, 64, 31,  2, 27, 27, 58, 82,  3,\n",
       "       27,  3, 42, 79, 82, 27, 74, 79, 52,  3, 71,  0, 74,  3, 46, 22, 27,\n",
       "        3, 79, 27,  3, 42,  9, 52, 46, 27, 79, 44, 52, 27, 60, 71,  9, 48,\n",
       "       42,  3, 27, 46, 64, 46, 77, 13,  2, 27, 27, 50, 46, 46, 22, 69, 77,\n",
       "        3, 27,  3, 42, 64, 27, 21,  9, 48, 42,  3, 69, 77, 27, 75, 21,  0,\n",
       "       37, 46, 27, 44,  9,  3, 42, 27, 77, 46, 21, 75, 10, 77, 82, 60, 77,\n",
       "        3,  0, 52,  3,  9,  0, 21, 27, 75, 82, 46, 21, 13,  2, 27, 27,  7,\n",
       "        0,  1,  9, 52, 48, 27,  0, 27, 75,  0, 37,  9, 52, 46, 27, 44, 42,\n",
       "       46, 71, 46, 27,  0, 60, 82, 52, 22,  0, 52, 74, 46, 27, 21,  9, 46,\n",
       "       77, 13,  2, 27, 27, 43, 42, 64, 27, 77, 46, 21, 75, 27,  3, 42, 64,\n",
       "       27, 75, 79, 46, 13, 27,  3, 79, 27,  3, 42, 64, 27, 77, 44, 46, 46,\n",
       "        3, 27, 77, 46, 21, 75, 27,  3, 79, 79, 27, 74, 71, 82, 46, 21, 31,\n",
       "        2, 27, 27, 43, 42, 79, 82, 27,  3, 42,  0,  3, 27,  0, 71,  3, 27,\n",
       "       52, 79, 44, 27,  3, 42, 46, 27, 44, 79, 71, 21, 22, 69, 77, 27, 75,\n",
       "       71, 46, 77, 42, 27, 79, 71, 52,  0, 37, 46, 52,  3, 13,  2, 27, 27,\n",
       "       38, 52, 22, 27, 79, 52, 21, 64, 27, 42, 46, 71,  0, 21, 22, 27,  3,\n",
       "       79, 27,  3, 42, 46, 27, 48,  0, 82, 22, 64, 27, 77, 32, 71,  9, 52,\n",
       "       48, 13,  2, 27, 27, 54,  9,  3, 42,  9, 52, 27,  3, 42,  9, 52, 46,\n",
       "       27, 79, 44, 52, 27, 60, 82])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_text = np.array([encoder[char] for char in text])\n",
    "encoder_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c2ef3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06807f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    \n",
    "    #encoded_text : batch of encoded text\n",
    "    #num_uni_chars : len(set(text))\n",
    "    \n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars)) #num_chars_in_batch(s) x unique_chars\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    one_hot[ np.arange(one_hot.shape[0]), encoded_text.flatten()]=1.0\n",
    "    #Resize to normal format\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1ff64be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_text.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da9b29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((9,3)) #Normal format 3x3\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b5dac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1,2,0]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88577403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[np.arange(a.shape[0]), b.flatten()] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0368d9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.reshape((*(3,3),3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c65b120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268a450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "def foo(bar,l):\n",
    "    print(bar,l)\n",
    "\n",
    "baz = [1, 2]\n",
    "\n",
    "foo(*baz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e0d8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1,2,0])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83e116b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b22cb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = np.arange(10)\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbcad975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape(5,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d77eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    #x : encoded_text of length seq_len\n",
    "    #y : encoded_text shifted by 1\n",
    "    char_per_batch = samp_per_batch * seq_len #total chars used per batch\n",
    "    num_batches_available = int(len(encoded_text) / char_per_batch)\n",
    "    encoded_text = encoded_text[:num_batches_available*char_per_batch] #limit the size of original text\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:,:-1] = x[:,1:]\n",
    "        try:\n",
    "            y[:,-1] = encoded_text[:, n+seq_len]\n",
    "        except:\n",
    "            y[:,-1] = encoded_text[:,0]\n",
    "        yield x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efe40636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       27, 27, 27])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = encoder_text[:20]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a3ab251",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text, samp_per_batch=2,seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2eb3c72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac196350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 27, 27, 27, 27],\n",
       "       [27, 27, 27, 27, 27]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4b27c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 27, 27, 27, 27],\n",
       "       [27, 27, 27, 27, 27]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a377802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, all_chars, num_hidden = 256, num_layers=3, drop_prob=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        self.drop_prob_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:ind for ind, char in decoder.items()}\n",
    "        \n",
    "        #Batch first : (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        return final_out, hidden\n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                      torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                      torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60a63391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(all_characters, num_hidden=512, num_layers=3, drop_prob=0.5, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c5ecfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = []\n",
    "for p in model.parameters():\n",
    "    total_params.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91c9f58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e7de1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1c58fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9\n",
    "train_ind = int(len(encoder_text)*train_percent)\n",
    "train_data = encoder_text[:train_ind] \n",
    "test_data = encoder_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9eeba567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901048"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6342bfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544561"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6740073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:0 Step:25 Val_loss:3.203792095184326\n",
      "EPOCH:0 Step:50 Val_loss:3.1922414302825928\n",
      "EPOCH:0 Step:75 Val_loss:3.1915736198425293\n",
      "EPOCH:0 Step:100 Val_loss:3.192152261734009\n",
      "EPOCH:0 Step:125 Val_loss:3.061629295349121\n",
      "EPOCH:0 Step:150 Val_loss:2.982011079788208\n",
      "EPOCH:0 Step:175 Val_loss:2.895754337310791\n",
      "EPOCH:0 Step:200 Val_loss:2.7833445072174072\n",
      "EPOCH:0 Step:225 Val_loss:2.7041707038879395\n",
      "EPOCH:0 Step:250 Val_loss:2.6285839080810547\n",
      "EPOCH:0 Step:275 Val_loss:2.534939765930176\n",
      "EPOCH:0 Step:300 Val_loss:2.429866313934326\n",
      "EPOCH:0 Step:325 Val_loss:2.356574296951294\n",
      "EPOCH:0 Step:350 Val_loss:2.283618211746216\n",
      "EPOCH:0 Step:375 Val_loss:2.2151834964752197\n",
      "EPOCH:0 Step:400 Val_loss:2.161541700363159\n",
      "EPOCH:0 Step:425 Val_loss:2.1264209747314453\n",
      "EPOCH:0 Step:450 Val_loss:2.081611156463623\n",
      "EPOCH:0 Step:475 Val_loss:2.041590929031372\n",
      "EPOCH:1 Step:500 Val_loss:2.010805368423462\n",
      "EPOCH:1 Step:525 Val_loss:1.9800060987472534\n",
      "EPOCH:1 Step:550 Val_loss:1.9530264139175415\n",
      "EPOCH:1 Step:575 Val_loss:1.9251234531402588\n",
      "EPOCH:1 Step:600 Val_loss:1.9066798686981201\n",
      "EPOCH:1 Step:625 Val_loss:1.8829902410507202\n",
      "EPOCH:1 Step:650 Val_loss:1.856163501739502\n",
      "EPOCH:1 Step:675 Val_loss:1.8413031101226807\n",
      "EPOCH:1 Step:700 Val_loss:1.8177390098571777\n",
      "EPOCH:1 Step:725 Val_loss:1.8023375272750854\n",
      "EPOCH:1 Step:750 Val_loss:1.7833542823791504\n",
      "EPOCH:1 Step:775 Val_loss:1.7710728645324707\n",
      "EPOCH:1 Step:800 Val_loss:1.752741813659668\n",
      "EPOCH:1 Step:825 Val_loss:1.7378857135772705\n",
      "EPOCH:1 Step:850 Val_loss:1.72121000289917\n",
      "EPOCH:1 Step:875 Val_loss:1.7118078470230103\n",
      "EPOCH:1 Step:900 Val_loss:1.6986980438232422\n",
      "EPOCH:1 Step:925 Val_loss:1.6884169578552246\n",
      "EPOCH:1 Step:950 Val_loss:1.6772181987762451\n",
      "EPOCH:1 Step:975 Val_loss:1.6618777513504028\n",
      "EPOCH:2 Step:1000 Val_loss:1.656849980354309\n",
      "EPOCH:2 Step:1025 Val_loss:1.6471775770187378\n",
      "EPOCH:2 Step:1050 Val_loss:1.6439881324768066\n",
      "EPOCH:2 Step:1075 Val_loss:1.6239532232284546\n",
      "EPOCH:2 Step:1100 Val_loss:1.6181648969650269\n",
      "EPOCH:2 Step:1125 Val_loss:1.6051654815673828\n",
      "EPOCH:2 Step:1150 Val_loss:1.6028952598571777\n",
      "EPOCH:2 Step:1175 Val_loss:1.5918570756912231\n",
      "EPOCH:2 Step:1200 Val_loss:1.5802720785140991\n",
      "EPOCH:2 Step:1225 Val_loss:1.575564980506897\n",
      "EPOCH:2 Step:1250 Val_loss:1.5689810514450073\n",
      "EPOCH:2 Step:1275 Val_loss:1.5681257247924805\n",
      "EPOCH:2 Step:1300 Val_loss:1.5565972328186035\n",
      "EPOCH:2 Step:1325 Val_loss:1.550575613975525\n",
      "EPOCH:2 Step:1350 Val_loss:1.5463454723358154\n",
      "EPOCH:2 Step:1375 Val_loss:1.5379217863082886\n",
      "EPOCH:2 Step:1400 Val_loss:1.5309072732925415\n",
      "EPOCH:2 Step:1425 Val_loss:1.5302162170410156\n",
      "EPOCH:2 Step:1450 Val_loss:1.5211275815963745\n",
      "EPOCH:3 Step:1475 Val_loss:1.525447964668274\n",
      "EPOCH:3 Step:1500 Val_loss:1.5140910148620605\n",
      "EPOCH:3 Step:1525 Val_loss:1.511421799659729\n",
      "EPOCH:3 Step:1550 Val_loss:1.5040056705474854\n",
      "EPOCH:3 Step:1575 Val_loss:1.4986331462860107\n",
      "EPOCH:3 Step:1600 Val_loss:1.4927258491516113\n",
      "EPOCH:3 Step:1625 Val_loss:1.4873428344726562\n",
      "EPOCH:3 Step:1650 Val_loss:1.4856828451156616\n",
      "EPOCH:3 Step:1675 Val_loss:1.4790441989898682\n",
      "EPOCH:3 Step:1700 Val_loss:1.473465919494629\n",
      "EPOCH:3 Step:1725 Val_loss:1.4718782901763916\n",
      "EPOCH:3 Step:1750 Val_loss:1.4719434976577759\n",
      "EPOCH:3 Step:1775 Val_loss:1.4728864431381226\n",
      "EPOCH:3 Step:1800 Val_loss:1.4617079496383667\n",
      "EPOCH:3 Step:1825 Val_loss:1.466978669166565\n",
      "EPOCH:3 Step:1850 Val_loss:1.4611948728561401\n",
      "EPOCH:3 Step:1875 Val_loss:1.4584717750549316\n",
      "EPOCH:3 Step:1900 Val_loss:1.4557336568832397\n",
      "EPOCH:3 Step:1925 Val_loss:1.4497287273406982\n",
      "EPOCH:3 Step:1950 Val_loss:1.4420162439346313\n",
      "EPOCH:4 Step:1975 Val_loss:1.442214012145996\n",
      "EPOCH:4 Step:2000 Val_loss:1.4437170028686523\n",
      "EPOCH:4 Step:2025 Val_loss:1.438111662864685\n",
      "EPOCH:4 Step:2050 Val_loss:1.4335048198699951\n",
      "EPOCH:4 Step:2075 Val_loss:1.4385212659835815\n",
      "EPOCH:4 Step:2100 Val_loss:1.4303994178771973\n",
      "EPOCH:4 Step:2125 Val_loss:1.428629755973816\n",
      "EPOCH:4 Step:2150 Val_loss:1.4233165979385376\n",
      "EPOCH:4 Step:2175 Val_loss:1.4199390411376953\n",
      "EPOCH:4 Step:2200 Val_loss:1.421069860458374\n",
      "EPOCH:4 Step:2225 Val_loss:1.4220775365829468\n",
      "EPOCH:4 Step:2250 Val_loss:1.4208638668060303\n",
      "EPOCH:4 Step:2275 Val_loss:1.4159938097000122\n",
      "EPOCH:4 Step:2300 Val_loss:1.4143075942993164\n",
      "EPOCH:4 Step:2325 Val_loss:1.4138731956481934\n",
      "EPOCH:4 Step:2350 Val_loss:1.4182976484298706\n",
      "EPOCH:4 Step:2375 Val_loss:1.4179354906082153\n",
      "EPOCH:4 Step:2400 Val_loss:1.4089051485061646\n",
      "EPOCH:4 Step:2425 Val_loss:1.4051111936569214\n",
      "EPOCH:4 Step:2450 Val_loss:1.4022722244262695\n",
      "EPOCH:5 Step:2475 Val_loss:1.4035251140594482\n",
      "EPOCH:5 Step:2500 Val_loss:1.4077082872390747\n",
      "EPOCH:5 Step:2525 Val_loss:1.4052938222885132\n",
      "EPOCH:5 Step:2550 Val_loss:1.4017959833145142\n",
      "EPOCH:5 Step:2575 Val_loss:1.3973585367202759\n",
      "EPOCH:5 Step:2600 Val_loss:1.3962444067001343\n",
      "EPOCH:5 Step:2625 Val_loss:1.394720196723938\n",
      "EPOCH:5 Step:2650 Val_loss:1.3914955854415894\n",
      "EPOCH:5 Step:2675 Val_loss:1.3869825601577759\n",
      "EPOCH:5 Step:2700 Val_loss:1.3851507902145386\n",
      "EPOCH:5 Step:2725 Val_loss:1.3894147872924805\n",
      "EPOCH:5 Step:2750 Val_loss:1.3865201473236084\n",
      "EPOCH:5 Step:2775 Val_loss:1.3824548721313477\n",
      "EPOCH:5 Step:2800 Val_loss:1.3911930322647095\n",
      "EPOCH:5 Step:2825 Val_loss:1.3826686143875122\n",
      "EPOCH:5 Step:2850 Val_loss:1.3874832391738892\n",
      "EPOCH:5 Step:2875 Val_loss:1.386627197265625\n",
      "EPOCH:5 Step:2900 Val_loss:1.386414885520935\n",
      "EPOCH:5 Step:2925 Val_loss:1.377503752708435\n",
      "EPOCH:6 Step:2950 Val_loss:1.3775663375854492\n",
      "EPOCH:6 Step:2975 Val_loss:1.380044937133789\n",
      "EPOCH:6 Step:3000 Val_loss:1.3773592710494995\n",
      "EPOCH:6 Step:3025 Val_loss:1.3778643608093262\n",
      "EPOCH:6 Step:3050 Val_loss:1.3764574527740479\n",
      "EPOCH:6 Step:3075 Val_loss:1.3666402101516724\n",
      "EPOCH:6 Step:3100 Val_loss:1.3757200241088867\n",
      "EPOCH:6 Step:3125 Val_loss:1.3674784898757935\n",
      "EPOCH:6 Step:3150 Val_loss:1.3687541484832764\n",
      "EPOCH:6 Step:3175 Val_loss:1.3649595975875854\n",
      "EPOCH:6 Step:3200 Val_loss:1.3649762868881226\n",
      "EPOCH:6 Step:3225 Val_loss:1.3648298978805542\n",
      "EPOCH:6 Step:3250 Val_loss:1.362394094467163\n",
      "EPOCH:6 Step:3275 Val_loss:1.3603606224060059\n",
      "EPOCH:6 Step:3300 Val_loss:1.3647695779800415\n",
      "EPOCH:6 Step:3325 Val_loss:1.3684003353118896\n",
      "EPOCH:6 Step:3350 Val_loss:1.365314245223999\n",
      "EPOCH:6 Step:3375 Val_loss:1.3649197816848755\n",
      "EPOCH:6 Step:3400 Val_loss:1.3635921478271484\n",
      "EPOCH:6 Step:3425 Val_loss:1.357888102531433\n",
      "EPOCH:7 Step:3450 Val_loss:1.3576610088348389\n",
      "EPOCH:7 Step:3475 Val_loss:1.35813570022583\n",
      "EPOCH:7 Step:3500 Val_loss:1.3573423624038696\n",
      "EPOCH:7 Step:3525 Val_loss:1.3527055978775024\n",
      "EPOCH:7 Step:3550 Val_loss:1.3566526174545288\n",
      "EPOCH:7 Step:3575 Val_loss:1.353532075881958\n",
      "EPOCH:7 Step:3600 Val_loss:1.3550692796707153\n",
      "EPOCH:7 Step:3625 Val_loss:1.3517630100250244\n",
      "EPOCH:7 Step:3650 Val_loss:1.3522757291793823\n",
      "EPOCH:7 Step:3675 Val_loss:1.3515956401824951\n",
      "EPOCH:7 Step:3700 Val_loss:1.352013349533081\n",
      "EPOCH:7 Step:3725 Val_loss:1.347841739654541\n",
      "EPOCH:7 Step:3750 Val_loss:1.3492846488952637\n",
      "EPOCH:7 Step:3775 Val_loss:1.3501474857330322\n",
      "EPOCH:7 Step:3800 Val_loss:1.3558013439178467\n",
      "EPOCH:7 Step:3825 Val_loss:1.358040452003479\n",
      "EPOCH:7 Step:3850 Val_loss:1.3557394742965698\n",
      "EPOCH:7 Step:3875 Val_loss:1.3522746562957764\n",
      "EPOCH:7 Step:3900 Val_loss:1.3459941148757935\n",
      "EPOCH:8 Step:3925 Val_loss:1.3507966995239258\n",
      "EPOCH:8 Step:3950 Val_loss:1.3461440801620483\n",
      "EPOCH:8 Step:3975 Val_loss:1.3490843772888184\n",
      "EPOCH:8 Step:4000 Val_loss:1.3473221063613892\n",
      "EPOCH:8 Step:4025 Val_loss:1.3474658727645874\n",
      "EPOCH:8 Step:4050 Val_loss:1.3345680236816406\n",
      "EPOCH:8 Step:4075 Val_loss:1.3390246629714966\n",
      "EPOCH:8 Step:4100 Val_loss:1.3400483131408691\n",
      "EPOCH:8 Step:4125 Val_loss:1.3367608785629272\n",
      "EPOCH:8 Step:4150 Val_loss:1.3371597528457642\n",
      "EPOCH:8 Step:4175 Val_loss:1.3376448154449463\n",
      "EPOCH:8 Step:4200 Val_loss:1.3401978015899658\n",
      "EPOCH:8 Step:4225 Val_loss:1.3352251052856445\n",
      "EPOCH:8 Step:4250 Val_loss:1.3346668481826782\n",
      "EPOCH:8 Step:4275 Val_loss:1.3427311182022095\n",
      "EPOCH:8 Step:4300 Val_loss:1.340196132659912\n",
      "EPOCH:8 Step:4325 Val_loss:1.3438671827316284\n",
      "EPOCH:8 Step:4350 Val_loss:1.342964768409729\n",
      "EPOCH:8 Step:4375 Val_loss:1.3400603532791138\n",
      "EPOCH:8 Step:4400 Val_loss:1.3342969417572021\n",
      "EPOCH:9 Step:4425 Val_loss:1.3325915336608887\n",
      "EPOCH:9 Step:4450 Val_loss:1.3350865840911865\n",
      "EPOCH:9 Step:4475 Val_loss:1.3321359157562256\n",
      "EPOCH:9 Step:4500 Val_loss:1.3319872617721558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:9 Step:4525 Val_loss:1.3372199535369873\n",
      "EPOCH:9 Step:4550 Val_loss:1.328216552734375\n",
      "EPOCH:9 Step:4575 Val_loss:1.3360037803649902\n",
      "EPOCH:9 Step:4600 Val_loss:1.3334412574768066\n",
      "EPOCH:9 Step:4625 Val_loss:1.3290234804153442\n",
      "EPOCH:9 Step:4650 Val_loss:1.3293308019638062\n",
      "EPOCH:9 Step:4675 Val_loss:1.3312005996704102\n",
      "EPOCH:9 Step:4700 Val_loss:1.3281508684158325\n",
      "EPOCH:9 Step:4725 Val_loss:1.3277826309204102\n",
      "EPOCH:9 Step:4750 Val_loss:1.329433798789978\n",
      "EPOCH:9 Step:4775 Val_loss:1.3333945274353027\n",
      "EPOCH:9 Step:4800 Val_loss:1.336094856262207\n",
      "EPOCH:9 Step:4825 Val_loss:1.3352025747299194\n",
      "EPOCH:9 Step:4850 Val_loss:1.330862283706665\n",
      "EPOCH:9 Step:4875 Val_loss:1.3307867050170898\n",
      "EPOCH:9 Step:4900 Val_loss:1.32881498336792\n",
      "EPOCH:10 Step:4925 Val_loss:1.3321460485458374\n",
      "EPOCH:10 Step:4950 Val_loss:1.3341333866119385\n",
      "EPOCH:10 Step:4975 Val_loss:1.329681396484375\n",
      "EPOCH:10 Step:5000 Val_loss:1.3314917087554932\n",
      "EPOCH:10 Step:5025 Val_loss:1.3284004926681519\n",
      "EPOCH:10 Step:5050 Val_loss:1.324951410293579\n",
      "EPOCH:10 Step:5075 Val_loss:1.3314892053604126\n",
      "EPOCH:10 Step:5100 Val_loss:1.33286714553833\n",
      "EPOCH:10 Step:5125 Val_loss:1.3283939361572266\n",
      "EPOCH:10 Step:5150 Val_loss:1.3226072788238525\n",
      "EPOCH:10 Step:5175 Val_loss:1.3240342140197754\n",
      "EPOCH:10 Step:5200 Val_loss:1.3298418521881104\n",
      "EPOCH:10 Step:5225 Val_loss:1.3204079866409302\n",
      "EPOCH:10 Step:5250 Val_loss:1.3295782804489136\n",
      "EPOCH:10 Step:5275 Val_loss:1.329436182975769\n",
      "EPOCH:10 Step:5300 Val_loss:1.3307945728302002\n",
      "EPOCH:10 Step:5325 Val_loss:1.3316824436187744\n",
      "EPOCH:10 Step:5350 Val_loss:1.3258577585220337\n",
      "EPOCH:10 Step:5375 Val_loss:1.3282382488250732\n",
      "EPOCH:11 Step:5400 Val_loss:1.3253451585769653\n",
      "EPOCH:11 Step:5425 Val_loss:1.3285655975341797\n",
      "EPOCH:11 Step:5450 Val_loss:1.3258545398712158\n",
      "EPOCH:11 Step:5475 Val_loss:1.321424126625061\n",
      "EPOCH:11 Step:5500 Val_loss:1.3247536420822144\n",
      "EPOCH:11 Step:5525 Val_loss:1.3161569833755493\n",
      "EPOCH:11 Step:5550 Val_loss:1.3227156400680542\n",
      "EPOCH:11 Step:5575 Val_loss:1.3217730522155762\n",
      "EPOCH:11 Step:5600 Val_loss:1.3188319206237793\n",
      "EPOCH:11 Step:5625 Val_loss:1.3146430253982544\n",
      "EPOCH:11 Step:5650 Val_loss:1.3144679069519043\n",
      "EPOCH:11 Step:5675 Val_loss:1.3152177333831787\n",
      "EPOCH:11 Step:5700 Val_loss:1.3146061897277832\n",
      "EPOCH:11 Step:5725 Val_loss:1.3124457597732544\n",
      "EPOCH:11 Step:5750 Val_loss:1.3230177164077759\n",
      "EPOCH:11 Step:5775 Val_loss:1.3220585584640503\n",
      "EPOCH:11 Step:5800 Val_loss:1.3276569843292236\n",
      "EPOCH:11 Step:5825 Val_loss:1.324051022529602\n",
      "EPOCH:11 Step:5850 Val_loss:1.3245710134506226\n",
      "EPOCH:11 Step:5875 Val_loss:1.31669282913208\n",
      "EPOCH:12 Step:5900 Val_loss:1.3202236890792847\n",
      "EPOCH:12 Step:5925 Val_loss:1.3176149129867554\n",
      "EPOCH:12 Step:5950 Val_loss:1.319379448890686\n",
      "EPOCH:12 Step:5975 Val_loss:1.3090978860855103\n",
      "EPOCH:12 Step:6000 Val_loss:1.3191291093826294\n",
      "EPOCH:12 Step:6025 Val_loss:1.3109407424926758\n",
      "EPOCH:12 Step:6050 Val_loss:1.317884087562561\n",
      "EPOCH:12 Step:6075 Val_loss:1.3155282735824585\n",
      "EPOCH:12 Step:6100 Val_loss:1.3126510381698608\n",
      "EPOCH:12 Step:6125 Val_loss:1.3118754625320435\n",
      "EPOCH:12 Step:6150 Val_loss:1.3119663000106812\n",
      "EPOCH:12 Step:6175 Val_loss:1.308517336845398\n",
      "EPOCH:12 Step:6200 Val_loss:1.3081830739974976\n",
      "EPOCH:12 Step:6225 Val_loss:1.3123787641525269\n",
      "EPOCH:12 Step:6250 Val_loss:1.3174735307693481\n",
      "EPOCH:12 Step:6275 Val_loss:1.315981388092041\n",
      "EPOCH:12 Step:6300 Val_loss:1.318979263305664\n",
      "EPOCH:12 Step:6325 Val_loss:1.315920114517212\n",
      "EPOCH:12 Step:6350 Val_loss:1.3160321712493896\n",
      "EPOCH:13 Step:6375 Val_loss:1.3144700527191162\n",
      "EPOCH:13 Step:6400 Val_loss:1.3166542053222656\n",
      "EPOCH:13 Step:6425 Val_loss:1.3179072141647339\n",
      "EPOCH:13 Step:6450 Val_loss:1.314711570739746\n",
      "EPOCH:13 Step:6475 Val_loss:1.3095512390136719\n",
      "EPOCH:13 Step:6500 Val_loss:1.3074389696121216\n",
      "EPOCH:13 Step:6525 Val_loss:1.3123457431793213\n",
      "EPOCH:13 Step:6550 Val_loss:1.3135058879852295\n",
      "EPOCH:13 Step:6575 Val_loss:1.3131160736083984\n",
      "EPOCH:13 Step:6600 Val_loss:1.3078727722167969\n",
      "EPOCH:13 Step:6625 Val_loss:1.303420066833496\n",
      "EPOCH:13 Step:6650 Val_loss:1.3106518983840942\n",
      "EPOCH:13 Step:6675 Val_loss:1.306029200553894\n",
      "EPOCH:13 Step:6700 Val_loss:1.3079558610916138\n",
      "EPOCH:13 Step:6725 Val_loss:1.3131928443908691\n",
      "EPOCH:13 Step:6750 Val_loss:1.312889575958252\n",
      "EPOCH:13 Step:6775 Val_loss:1.3165032863616943\n",
      "EPOCH:13 Step:6800 Val_loss:1.3165556192398071\n",
      "EPOCH:13 Step:6825 Val_loss:1.318183422088623\n",
      "EPOCH:13 Step:6850 Val_loss:1.3151755332946777\n",
      "EPOCH:14 Step:6875 Val_loss:1.3133314847946167\n",
      "EPOCH:14 Step:6900 Val_loss:1.320430040359497\n",
      "EPOCH:14 Step:6925 Val_loss:1.3161296844482422\n",
      "EPOCH:14 Step:6950 Val_loss:1.3168845176696777\n",
      "EPOCH:14 Step:6975 Val_loss:1.3173177242279053\n",
      "EPOCH:14 Step:7000 Val_loss:1.3088656663894653\n",
      "EPOCH:14 Step:7025 Val_loss:1.3158140182495117\n",
      "EPOCH:14 Step:7050 Val_loss:1.3069251775741577\n",
      "EPOCH:14 Step:7075 Val_loss:1.3100415468215942\n",
      "EPOCH:14 Step:7100 Val_loss:1.30966055393219\n",
      "EPOCH:14 Step:7125 Val_loss:1.3035577535629272\n",
      "EPOCH:14 Step:7150 Val_loss:1.3051084280014038\n",
      "EPOCH:14 Step:7175 Val_loss:1.2992359399795532\n",
      "EPOCH:14 Step:7200 Val_loss:1.306321620941162\n",
      "EPOCH:14 Step:7225 Val_loss:1.3114279508590698\n",
      "EPOCH:14 Step:7250 Val_loss:1.3157429695129395\n",
      "EPOCH:14 Step:7275 Val_loss:1.3160374164581299\n",
      "EPOCH:14 Step:7300 Val_loss:1.3153035640716553\n",
      "EPOCH:14 Step:7325 Val_loss:1.315158486366272\n",
      "EPOCH:14 Step:7350 Val_loss:1.3103340864181519\n",
      "EPOCH:15 Step:7375 Val_loss:1.3138213157653809\n",
      "EPOCH:15 Step:7400 Val_loss:1.317961573600769\n",
      "EPOCH:15 Step:7425 Val_loss:1.31800377368927\n",
      "EPOCH:15 Step:7450 Val_loss:1.3171056509017944\n",
      "EPOCH:15 Step:7475 Val_loss:1.314104437828064\n",
      "EPOCH:15 Step:7500 Val_loss:1.3090726137161255\n",
      "EPOCH:15 Step:7525 Val_loss:1.3145934343338013\n",
      "EPOCH:15 Step:7550 Val_loss:1.3117245435714722\n",
      "EPOCH:15 Step:7575 Val_loss:1.3107409477233887\n",
      "EPOCH:15 Step:7600 Val_loss:1.3046386241912842\n",
      "EPOCH:15 Step:7625 Val_loss:1.3077651262283325\n",
      "EPOCH:15 Step:7650 Val_loss:1.3061290979385376\n",
      "EPOCH:15 Step:7675 Val_loss:1.303069829940796\n",
      "EPOCH:15 Step:7700 Val_loss:1.3103975057601929\n",
      "EPOCH:15 Step:7725 Val_loss:1.3106646537780762\n",
      "EPOCH:15 Step:7750 Val_loss:1.3147250413894653\n",
      "EPOCH:15 Step:7775 Val_loss:1.322023630142212\n",
      "EPOCH:15 Step:7800 Val_loss:1.3129799365997314\n",
      "EPOCH:15 Step:7825 Val_loss:1.3125278949737549\n",
      "EPOCH:16 Step:7850 Val_loss:1.3102660179138184\n",
      "EPOCH:16 Step:7875 Val_loss:1.30979585647583\n",
      "EPOCH:16 Step:7900 Val_loss:1.3114374876022339\n",
      "EPOCH:16 Step:7925 Val_loss:1.309146523475647\n",
      "EPOCH:16 Step:7950 Val_loss:1.3146029710769653\n",
      "EPOCH:16 Step:7975 Val_loss:1.3049302101135254\n",
      "EPOCH:16 Step:8000 Val_loss:1.310999870300293\n",
      "EPOCH:16 Step:8025 Val_loss:1.3089500665664673\n",
      "EPOCH:16 Step:8050 Val_loss:1.3057609796524048\n",
      "EPOCH:16 Step:8075 Val_loss:1.3074973821640015\n",
      "EPOCH:16 Step:8100 Val_loss:1.2996786832809448\n",
      "EPOCH:16 Step:8125 Val_loss:1.2980140447616577\n",
      "EPOCH:16 Step:8150 Val_loss:1.3019810914993286\n",
      "EPOCH:16 Step:8175 Val_loss:1.2969192266464233\n",
      "EPOCH:16 Step:8200 Val_loss:1.3057183027267456\n",
      "EPOCH:16 Step:8225 Val_loss:1.3077569007873535\n",
      "EPOCH:16 Step:8250 Val_loss:1.3146271705627441\n",
      "EPOCH:16 Step:8275 Val_loss:1.3073104619979858\n",
      "EPOCH:16 Step:8300 Val_loss:1.3127541542053223\n",
      "EPOCH:16 Step:8325 Val_loss:1.3032824993133545\n",
      "EPOCH:17 Step:8350 Val_loss:1.3054479360580444\n",
      "EPOCH:17 Step:8375 Val_loss:1.3078004121780396\n",
      "EPOCH:17 Step:8400 Val_loss:1.306427240371704\n",
      "EPOCH:17 Step:8425 Val_loss:1.3030741214752197\n",
      "EPOCH:17 Step:8450 Val_loss:1.3049134016036987\n",
      "EPOCH:17 Step:8475 Val_loss:1.2961713075637817\n",
      "EPOCH:17 Step:8500 Val_loss:1.3004992008209229\n",
      "EPOCH:17 Step:8525 Val_loss:1.3051550388336182\n",
      "EPOCH:17 Step:8550 Val_loss:1.304917335510254\n",
      "EPOCH:17 Step:8575 Val_loss:1.2966070175170898\n",
      "EPOCH:17 Step:8600 Val_loss:1.3006620407104492\n",
      "EPOCH:17 Step:8625 Val_loss:1.2950901985168457\n",
      "EPOCH:17 Step:8650 Val_loss:1.2967267036437988\n",
      "EPOCH:17 Step:8675 Val_loss:1.2978525161743164\n",
      "EPOCH:17 Step:8700 Val_loss:1.3036731481552124\n",
      "EPOCH:17 Step:8725 Val_loss:1.3040428161621094\n",
      "EPOCH:17 Step:8750 Val_loss:1.309889316558838\n",
      "EPOCH:17 Step:8775 Val_loss:1.307486653327942\n",
      "EPOCH:17 Step:8800 Val_loss:1.307175636291504\n",
      "EPOCH:18 Step:8825 Val_loss:1.3069242238998413\n",
      "EPOCH:18 Step:8850 Val_loss:1.30824613571167\n",
      "EPOCH:18 Step:8875 Val_loss:1.3076786994934082\n",
      "EPOCH:18 Step:8900 Val_loss:1.308213233947754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:18 Step:8925 Val_loss:1.3079910278320312\n",
      "EPOCH:18 Step:8950 Val_loss:1.296520709991455\n",
      "EPOCH:18 Step:8975 Val_loss:1.2997502088546753\n",
      "EPOCH:18 Step:9000 Val_loss:1.3057318925857544\n",
      "EPOCH:18 Step:9025 Val_loss:1.3056557178497314\n",
      "EPOCH:18 Step:9050 Val_loss:1.308991551399231\n",
      "EPOCH:18 Step:9075 Val_loss:1.2994381189346313\n",
      "EPOCH:18 Step:9100 Val_loss:1.3026940822601318\n",
      "EPOCH:18 Step:9125 Val_loss:1.3026379346847534\n",
      "EPOCH:18 Step:9150 Val_loss:1.3018511533737183\n",
      "EPOCH:18 Step:9175 Val_loss:1.3060081005096436\n",
      "EPOCH:18 Step:9200 Val_loss:1.303269863128662\n",
      "EPOCH:18 Step:9225 Val_loss:1.308787226676941\n",
      "EPOCH:18 Step:9250 Val_loss:1.3066060543060303\n",
      "EPOCH:18 Step:9275 Val_loss:1.3056020736694336\n",
      "EPOCH:18 Step:9300 Val_loss:1.3045344352722168\n",
      "EPOCH:19 Step:9325 Val_loss:1.3029792308807373\n",
      "EPOCH:19 Step:9350 Val_loss:1.302891731262207\n",
      "EPOCH:19 Step:9375 Val_loss:1.3066977262496948\n",
      "EPOCH:19 Step:9400 Val_loss:1.3039144277572632\n",
      "EPOCH:19 Step:9425 Val_loss:1.3095840215682983\n",
      "EPOCH:19 Step:9450 Val_loss:1.3022679090499878\n",
      "EPOCH:19 Step:9475 Val_loss:1.3059171438217163\n",
      "EPOCH:19 Step:9500 Val_loss:1.306768774986267\n",
      "EPOCH:19 Step:9525 Val_loss:1.3072128295898438\n",
      "EPOCH:19 Step:9550 Val_loss:1.3048100471496582\n",
      "EPOCH:19 Step:9575 Val_loss:1.3008253574371338\n",
      "EPOCH:19 Step:9600 Val_loss:1.303051233291626\n",
      "EPOCH:19 Step:9625 Val_loss:1.2988003492355347\n",
      "EPOCH:19 Step:9650 Val_loss:1.3033077716827393\n",
      "EPOCH:19 Step:9675 Val_loss:1.3109679222106934\n",
      "EPOCH:19 Step:9700 Val_loss:1.310575246810913\n",
      "EPOCH:19 Step:9725 Val_loss:1.3127247095108032\n",
      "EPOCH:19 Step:9750 Val_loss:1.3100378513336182\n",
      "EPOCH:19 Step:9775 Val_loss:1.312147617340088\n",
      "EPOCH:19 Step:9800 Val_loss:1.301974892616272\n",
      "EPOCH:20 Step:9825 Val_loss:1.3098565340042114\n",
      "EPOCH:20 Step:9850 Val_loss:1.3146138191223145\n",
      "EPOCH:20 Step:9875 Val_loss:1.3145982027053833\n",
      "EPOCH:20 Step:9900 Val_loss:1.3068642616271973\n",
      "EPOCH:20 Step:9925 Val_loss:1.3064191341400146\n",
      "EPOCH:20 Step:9950 Val_loss:1.2960312366485596\n",
      "EPOCH:20 Step:9975 Val_loss:1.302629828453064\n",
      "EPOCH:20 Step:10000 Val_loss:1.3077142238616943\n",
      "EPOCH:20 Step:10025 Val_loss:1.3040176630020142\n",
      "EPOCH:20 Step:10050 Val_loss:1.298737645149231\n",
      "EPOCH:20 Step:10075 Val_loss:1.3026601076126099\n",
      "EPOCH:20 Step:10100 Val_loss:1.3007757663726807\n",
      "EPOCH:20 Step:10125 Val_loss:1.2972333431243896\n",
      "EPOCH:20 Step:10150 Val_loss:1.3013399839401245\n",
      "EPOCH:20 Step:10175 Val_loss:1.3023133277893066\n",
      "EPOCH:20 Step:10200 Val_loss:1.3013461828231812\n",
      "EPOCH:20 Step:10225 Val_loss:1.3032211065292358\n",
      "EPOCH:20 Step:10250 Val_loss:1.3037792444229126\n",
      "EPOCH:20 Step:10275 Val_loss:1.3006470203399658\n",
      "EPOCH:21 Step:10300 Val_loss:1.2995105981826782\n",
      "EPOCH:21 Step:10325 Val_loss:1.3045368194580078\n",
      "EPOCH:21 Step:10350 Val_loss:1.3060718774795532\n",
      "EPOCH:21 Step:10375 Val_loss:1.3021878004074097\n",
      "EPOCH:21 Step:10400 Val_loss:1.3056992292404175\n",
      "EPOCH:21 Step:10425 Val_loss:1.2965143918991089\n",
      "EPOCH:21 Step:10450 Val_loss:1.2963579893112183\n",
      "EPOCH:21 Step:10475 Val_loss:1.2998781204223633\n",
      "EPOCH:21 Step:10500 Val_loss:1.3030277490615845\n",
      "EPOCH:21 Step:10525 Val_loss:1.2986631393432617\n",
      "EPOCH:21 Step:10550 Val_loss:1.2946163415908813\n",
      "EPOCH:21 Step:10575 Val_loss:1.298615574836731\n",
      "EPOCH:21 Step:10600 Val_loss:1.2949650287628174\n",
      "EPOCH:21 Step:10625 Val_loss:1.2971094846725464\n",
      "EPOCH:21 Step:10650 Val_loss:1.3043193817138672\n",
      "EPOCH:21 Step:10675 Val_loss:1.3000893592834473\n",
      "EPOCH:21 Step:10700 Val_loss:1.305441975593567\n",
      "EPOCH:21 Step:10725 Val_loss:1.3053075075149536\n",
      "EPOCH:21 Step:10750 Val_loss:1.303808569908142\n",
      "EPOCH:21 Step:10775 Val_loss:1.2989068031311035\n",
      "EPOCH:22 Step:10800 Val_loss:1.2966965436935425\n",
      "EPOCH:22 Step:10825 Val_loss:1.3062527179718018\n",
      "EPOCH:22 Step:10850 Val_loss:1.3047378063201904\n",
      "EPOCH:22 Step:10875 Val_loss:1.2957836389541626\n",
      "EPOCH:22 Step:10900 Val_loss:1.3008637428283691\n",
      "EPOCH:22 Step:10925 Val_loss:1.2971571683883667\n",
      "EPOCH:22 Step:10950 Val_loss:1.3066322803497314\n",
      "EPOCH:22 Step:10975 Val_loss:1.3049672842025757\n",
      "EPOCH:22 Step:11000 Val_loss:1.3078219890594482\n",
      "EPOCH:22 Step:11025 Val_loss:1.2977904081344604\n",
      "EPOCH:22 Step:11050 Val_loss:1.3008819818496704\n",
      "EPOCH:22 Step:11075 Val_loss:1.3023381233215332\n",
      "EPOCH:22 Step:11100 Val_loss:1.2959398031234741\n",
      "EPOCH:22 Step:11125 Val_loss:1.3021739721298218\n",
      "EPOCH:22 Step:11150 Val_loss:1.3016986846923828\n",
      "EPOCH:22 Step:11175 Val_loss:1.3030120134353638\n",
      "EPOCH:22 Step:11200 Val_loss:1.306180715560913\n",
      "EPOCH:22 Step:11225 Val_loss:1.298736572265625\n",
      "EPOCH:22 Step:11250 Val_loss:1.2959270477294922\n",
      "EPOCH:23 Step:11275 Val_loss:1.3005664348602295\n",
      "EPOCH:23 Step:11300 Val_loss:1.3043487071990967\n",
      "EPOCH:23 Step:11325 Val_loss:1.305641770362854\n",
      "EPOCH:23 Step:11350 Val_loss:1.3063496351242065\n",
      "EPOCH:23 Step:11375 Val_loss:1.300720453262329\n",
      "EPOCH:23 Step:11400 Val_loss:1.3019518852233887\n",
      "EPOCH:23 Step:11425 Val_loss:1.302212119102478\n",
      "EPOCH:23 Step:11450 Val_loss:1.3017072677612305\n",
      "EPOCH:23 Step:11475 Val_loss:1.304518699645996\n",
      "EPOCH:23 Step:11500 Val_loss:1.3025773763656616\n",
      "EPOCH:23 Step:11525 Val_loss:1.2940359115600586\n",
      "EPOCH:23 Step:11550 Val_loss:1.2978099584579468\n",
      "EPOCH:23 Step:11575 Val_loss:1.2952877283096313\n",
      "EPOCH:23 Step:11600 Val_loss:1.296965479850769\n",
      "EPOCH:23 Step:11625 Val_loss:1.3025909662246704\n",
      "EPOCH:23 Step:11650 Val_loss:1.2988274097442627\n",
      "EPOCH:23 Step:11675 Val_loss:1.304148554801941\n",
      "EPOCH:23 Step:11700 Val_loss:1.306625247001648\n",
      "EPOCH:23 Step:11725 Val_loss:1.304627537727356\n",
      "EPOCH:23 Step:11750 Val_loss:1.2952409982681274\n",
      "EPOCH:24 Step:11775 Val_loss:1.2983417510986328\n",
      "EPOCH:24 Step:11800 Val_loss:1.3039956092834473\n",
      "EPOCH:24 Step:11825 Val_loss:1.3014212846755981\n",
      "EPOCH:24 Step:11850 Val_loss:1.3040249347686768\n",
      "EPOCH:24 Step:11875 Val_loss:1.300669550895691\n",
      "EPOCH:24 Step:11900 Val_loss:1.2979393005371094\n",
      "EPOCH:24 Step:11925 Val_loss:1.2996890544891357\n",
      "EPOCH:24 Step:11950 Val_loss:1.3057509660720825\n",
      "EPOCH:24 Step:11975 Val_loss:1.2994405031204224\n",
      "EPOCH:24 Step:12000 Val_loss:1.2966277599334717\n",
      "EPOCH:24 Step:12025 Val_loss:1.2943998575210571\n",
      "EPOCH:24 Step:12050 Val_loss:1.2973965406417847\n",
      "EPOCH:24 Step:12075 Val_loss:1.297363042831421\n",
      "EPOCH:24 Step:12100 Val_loss:1.2963756322860718\n",
      "EPOCH:24 Step:12125 Val_loss:1.3045194149017334\n",
      "EPOCH:24 Step:12150 Val_loss:1.3064231872558594\n",
      "EPOCH:24 Step:12175 Val_loss:1.3070844411849976\n",
      "EPOCH:24 Step:12200 Val_loss:1.3030492067337036\n",
      "EPOCH:24 Step:12225 Val_loss:1.3029917478561401\n",
      "EPOCH:24 Step:12250 Val_loss:1.300861120223999\n",
      "EPOCH:25 Step:12275 Val_loss:1.301956295967102\n",
      "EPOCH:25 Step:12300 Val_loss:1.3013737201690674\n",
      "EPOCH:25 Step:12325 Val_loss:1.3070518970489502\n",
      "EPOCH:25 Step:12350 Val_loss:1.299377202987671\n",
      "EPOCH:25 Step:12375 Val_loss:1.2984888553619385\n",
      "EPOCH:25 Step:12400 Val_loss:1.3006287813186646\n",
      "EPOCH:25 Step:12425 Val_loss:1.3034499883651733\n",
      "EPOCH:25 Step:12450 Val_loss:1.3056851625442505\n",
      "EPOCH:25 Step:12475 Val_loss:1.3013031482696533\n",
      "EPOCH:25 Step:12500 Val_loss:1.2948307991027832\n",
      "EPOCH:25 Step:12525 Val_loss:1.2997725009918213\n",
      "EPOCH:25 Step:12550 Val_loss:1.2981290817260742\n",
      "EPOCH:25 Step:12575 Val_loss:1.2949496507644653\n",
      "EPOCH:25 Step:12600 Val_loss:1.2995712757110596\n",
      "EPOCH:25 Step:12625 Val_loss:1.2988277673721313\n",
      "EPOCH:25 Step:12650 Val_loss:1.2964750528335571\n",
      "EPOCH:25 Step:12675 Val_loss:1.298967719078064\n",
      "EPOCH:25 Step:12700 Val_loss:1.3015177249908447\n",
      "EPOCH:25 Step:12725 Val_loss:1.3002327680587769\n",
      "EPOCH:26 Step:12750 Val_loss:1.2955962419509888\n",
      "EPOCH:26 Step:12775 Val_loss:1.296695590019226\n",
      "EPOCH:26 Step:12800 Val_loss:1.3029659986495972\n",
      "EPOCH:26 Step:12825 Val_loss:1.299842357635498\n",
      "EPOCH:26 Step:12850 Val_loss:1.3014302253723145\n",
      "EPOCH:26 Step:12875 Val_loss:1.3006738424301147\n",
      "EPOCH:26 Step:12900 Val_loss:1.3034197092056274\n",
      "EPOCH:26 Step:12925 Val_loss:1.3047772645950317\n",
      "EPOCH:26 Step:12950 Val_loss:1.3018531799316406\n",
      "EPOCH:26 Step:12975 Val_loss:1.2969051599502563\n",
      "EPOCH:26 Step:13000 Val_loss:1.2918604612350464\n",
      "EPOCH:26 Step:13025 Val_loss:1.2942216396331787\n",
      "EPOCH:26 Step:13050 Val_loss:1.2894747257232666\n",
      "EPOCH:26 Step:13075 Val_loss:1.2908339500427246\n",
      "EPOCH:26 Step:13100 Val_loss:1.2966582775115967\n",
      "EPOCH:26 Step:13125 Val_loss:1.2965290546417236\n",
      "EPOCH:26 Step:13150 Val_loss:1.3058687448501587\n",
      "EPOCH:26 Step:13175 Val_loss:1.303174376487732\n",
      "EPOCH:26 Step:13200 Val_loss:1.300881028175354\n",
      "EPOCH:26 Step:13225 Val_loss:1.297905683517456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:27 Step:13250 Val_loss:1.295517086982727\n",
      "EPOCH:27 Step:13275 Val_loss:1.2996731996536255\n",
      "EPOCH:27 Step:13300 Val_loss:1.2996991872787476\n",
      "EPOCH:27 Step:13325 Val_loss:1.299917459487915\n",
      "EPOCH:27 Step:13350 Val_loss:1.2988203763961792\n",
      "EPOCH:27 Step:13375 Val_loss:1.2920480966567993\n",
      "EPOCH:27 Step:13400 Val_loss:1.2950773239135742\n",
      "EPOCH:27 Step:13425 Val_loss:1.2984477281570435\n",
      "EPOCH:27 Step:13450 Val_loss:1.2962560653686523\n",
      "EPOCH:27 Step:13475 Val_loss:1.2962371110916138\n",
      "EPOCH:27 Step:13500 Val_loss:1.2922207117080688\n",
      "EPOCH:27 Step:13525 Val_loss:1.285462737083435\n",
      "EPOCH:27 Step:13550 Val_loss:1.2887920141220093\n",
      "EPOCH:27 Step:13575 Val_loss:1.2936649322509766\n",
      "EPOCH:27 Step:13600 Val_loss:1.2960940599441528\n",
      "EPOCH:27 Step:13625 Val_loss:1.2984123229980469\n",
      "EPOCH:27 Step:13650 Val_loss:1.297707200050354\n",
      "EPOCH:27 Step:13675 Val_loss:1.2961639165878296\n",
      "EPOCH:27 Step:13700 Val_loss:1.2961616516113281\n",
      "EPOCH:28 Step:13725 Val_loss:1.2991211414337158\n",
      "EPOCH:28 Step:13750 Val_loss:1.2989100217819214\n",
      "EPOCH:28 Step:13775 Val_loss:1.2968473434448242\n",
      "EPOCH:28 Step:13800 Val_loss:1.2983593940734863\n",
      "EPOCH:28 Step:13825 Val_loss:1.2956182956695557\n",
      "EPOCH:28 Step:13850 Val_loss:1.2977920770645142\n",
      "EPOCH:28 Step:13875 Val_loss:1.2924138307571411\n",
      "EPOCH:28 Step:13900 Val_loss:1.2995920181274414\n",
      "EPOCH:28 Step:13925 Val_loss:1.3017338514328003\n",
      "EPOCH:28 Step:13950 Val_loss:1.2951560020446777\n",
      "EPOCH:28 Step:13975 Val_loss:1.2917717695236206\n",
      "EPOCH:28 Step:14000 Val_loss:1.2918049097061157\n",
      "EPOCH:28 Step:14025 Val_loss:1.290383219718933\n",
      "EPOCH:28 Step:14050 Val_loss:1.2933275699615479\n",
      "EPOCH:28 Step:14075 Val_loss:1.2921271324157715\n",
      "EPOCH:28 Step:14100 Val_loss:1.2951829433441162\n",
      "EPOCH:28 Step:14125 Val_loss:1.2923336029052734\n",
      "EPOCH:28 Step:14150 Val_loss:1.2981009483337402\n",
      "EPOCH:28 Step:14175 Val_loss:1.301456093788147\n",
      "EPOCH:28 Step:14200 Val_loss:1.2926870584487915\n",
      "EPOCH:29 Step:14225 Val_loss:1.2904983758926392\n",
      "EPOCH:29 Step:14250 Val_loss:1.2930500507354736\n",
      "EPOCH:29 Step:14275 Val_loss:1.2953364849090576\n",
      "EPOCH:29 Step:14300 Val_loss:1.2964993715286255\n",
      "EPOCH:29 Step:14325 Val_loss:1.298462152481079\n",
      "EPOCH:29 Step:14350 Val_loss:1.2926249504089355\n",
      "EPOCH:29 Step:14375 Val_loss:1.3011887073516846\n",
      "EPOCH:29 Step:14400 Val_loss:1.3040270805358887\n",
      "EPOCH:29 Step:14425 Val_loss:1.3033024072647095\n",
      "EPOCH:29 Step:14450 Val_loss:1.2987165451049805\n",
      "EPOCH:29 Step:14475 Val_loss:1.2940216064453125\n",
      "EPOCH:29 Step:14500 Val_loss:1.2954208850860596\n",
      "EPOCH:29 Step:14525 Val_loss:1.2928848266601562\n",
      "EPOCH:29 Step:14550 Val_loss:1.2869914770126343\n",
      "EPOCH:29 Step:14575 Val_loss:1.2933318614959717\n",
      "EPOCH:29 Step:14600 Val_loss:1.2908284664154053\n",
      "EPOCH:29 Step:14625 Val_loss:1.297561764717102\n",
      "EPOCH:29 Step:14650 Val_loss:1.297412395477295\n",
      "EPOCH:29 Step:14675 Val_loss:1.2975493669509888\n",
      "EPOCH:29 Step:14700 Val_loss:1.2911310195922852\n",
      "EPOCH:30 Step:14725 Val_loss:1.292940616607666\n",
      "EPOCH:30 Step:14750 Val_loss:1.296599268913269\n",
      "EPOCH:30 Step:14775 Val_loss:1.302475094795227\n",
      "EPOCH:30 Step:14800 Val_loss:1.290897011756897\n",
      "EPOCH:30 Step:14825 Val_loss:1.288082242012024\n",
      "EPOCH:30 Step:14850 Val_loss:1.293447494506836\n",
      "EPOCH:30 Step:14875 Val_loss:1.2918123006820679\n",
      "EPOCH:30 Step:14900 Val_loss:1.2989156246185303\n",
      "EPOCH:30 Step:14925 Val_loss:1.2940400838851929\n",
      "EPOCH:30 Step:14950 Val_loss:1.2888802289962769\n",
      "EPOCH:30 Step:14975 Val_loss:1.2980005741119385\n",
      "EPOCH:30 Step:15000 Val_loss:1.2948051691055298\n",
      "EPOCH:30 Step:15025 Val_loss:1.292911410331726\n",
      "EPOCH:30 Step:15050 Val_loss:1.3043982982635498\n",
      "EPOCH:30 Step:15075 Val_loss:1.300695776939392\n",
      "EPOCH:30 Step:15100 Val_loss:1.3001166582107544\n",
      "EPOCH:30 Step:15125 Val_loss:1.299452781677246\n",
      "EPOCH:30 Step:15150 Val_loss:1.2975150346755981\n",
      "EPOCH:30 Step:15175 Val_loss:1.294352412223816\n",
      "EPOCH:31 Step:15200 Val_loss:1.2915430068969727\n",
      "EPOCH:31 Step:15225 Val_loss:1.2992539405822754\n",
      "EPOCH:31 Step:15250 Val_loss:1.2963191270828247\n",
      "EPOCH:31 Step:15275 Val_loss:1.2966582775115967\n",
      "EPOCH:31 Step:15300 Val_loss:1.296553611755371\n",
      "EPOCH:31 Step:15325 Val_loss:1.2973021268844604\n",
      "EPOCH:31 Step:15350 Val_loss:1.2997794151306152\n",
      "EPOCH:31 Step:15375 Val_loss:1.2970495223999023\n",
      "EPOCH:31 Step:15400 Val_loss:1.2951788902282715\n",
      "EPOCH:31 Step:15425 Val_loss:1.293625831604004\n",
      "EPOCH:31 Step:15450 Val_loss:1.289201021194458\n",
      "EPOCH:31 Step:15475 Val_loss:1.2931970357894897\n",
      "EPOCH:31 Step:15500 Val_loss:1.2888797521591187\n",
      "EPOCH:31 Step:15525 Val_loss:1.2896369695663452\n",
      "EPOCH:31 Step:15550 Val_loss:1.2960211038589478\n",
      "EPOCH:31 Step:15575 Val_loss:1.2988734245300293\n",
      "EPOCH:31 Step:15600 Val_loss:1.3029533624649048\n",
      "EPOCH:31 Step:15625 Val_loss:1.3013702630996704\n",
      "EPOCH:31 Step:15650 Val_loss:1.3013949394226074\n",
      "EPOCH:31 Step:15675 Val_loss:1.2954596281051636\n",
      "EPOCH:32 Step:15700 Val_loss:1.293992042541504\n",
      "EPOCH:32 Step:15725 Val_loss:1.2953330278396606\n",
      "EPOCH:32 Step:15750 Val_loss:1.2986161708831787\n",
      "EPOCH:32 Step:15775 Val_loss:1.295547604560852\n",
      "EPOCH:32 Step:15800 Val_loss:1.2894177436828613\n",
      "EPOCH:32 Step:15825 Val_loss:1.286495327949524\n",
      "EPOCH:32 Step:15850 Val_loss:1.2904558181762695\n",
      "EPOCH:32 Step:15875 Val_loss:1.2970713376998901\n",
      "EPOCH:32 Step:15900 Val_loss:1.2964107990264893\n",
      "EPOCH:32 Step:15925 Val_loss:1.2898160219192505\n",
      "EPOCH:32 Step:15950 Val_loss:1.292662501335144\n",
      "EPOCH:32 Step:15975 Val_loss:1.2880775928497314\n",
      "EPOCH:32 Step:16000 Val_loss:1.2856162786483765\n",
      "EPOCH:32 Step:16025 Val_loss:1.289487600326538\n",
      "EPOCH:32 Step:16050 Val_loss:1.2949156761169434\n",
      "EPOCH:32 Step:16075 Val_loss:1.2950814962387085\n",
      "EPOCH:32 Step:16100 Val_loss:1.2942291498184204\n",
      "EPOCH:32 Step:16125 Val_loss:1.2990323305130005\n",
      "EPOCH:32 Step:16150 Val_loss:1.2974448204040527\n",
      "EPOCH:33 Step:16175 Val_loss:1.29452383518219\n",
      "EPOCH:33 Step:16200 Val_loss:1.299009919166565\n",
      "EPOCH:33 Step:16225 Val_loss:1.2952629327774048\n",
      "EPOCH:33 Step:16250 Val_loss:1.2968963384628296\n",
      "EPOCH:33 Step:16275 Val_loss:1.294987440109253\n",
      "EPOCH:33 Step:16300 Val_loss:1.2882535457611084\n",
      "EPOCH:33 Step:16325 Val_loss:1.2915154695510864\n",
      "EPOCH:33 Step:16350 Val_loss:1.2935175895690918\n",
      "EPOCH:33 Step:16375 Val_loss:1.2966575622558594\n",
      "EPOCH:33 Step:16400 Val_loss:1.2914515733718872\n",
      "EPOCH:33 Step:16425 Val_loss:1.2848782539367676\n",
      "EPOCH:33 Step:16450 Val_loss:1.2897546291351318\n",
      "EPOCH:33 Step:16475 Val_loss:1.2909090518951416\n",
      "EPOCH:33 Step:16500 Val_loss:1.2886090278625488\n",
      "EPOCH:33 Step:16525 Val_loss:1.2924751043319702\n",
      "EPOCH:33 Step:16550 Val_loss:1.2970950603485107\n",
      "EPOCH:33 Step:16575 Val_loss:1.2929298877716064\n",
      "EPOCH:33 Step:16600 Val_loss:1.2942324876785278\n",
      "EPOCH:33 Step:16625 Val_loss:1.2969329357147217\n",
      "EPOCH:33 Step:16650 Val_loss:1.296242356300354\n",
      "EPOCH:34 Step:16675 Val_loss:1.294224739074707\n",
      "EPOCH:34 Step:16700 Val_loss:1.2973051071166992\n",
      "EPOCH:34 Step:16725 Val_loss:1.2979224920272827\n",
      "EPOCH:34 Step:16750 Val_loss:1.2938565015792847\n",
      "EPOCH:34 Step:16775 Val_loss:1.2936798334121704\n",
      "EPOCH:34 Step:16800 Val_loss:1.2923924922943115\n",
      "EPOCH:34 Step:16825 Val_loss:1.2950031757354736\n",
      "EPOCH:34 Step:16850 Val_loss:1.2942944765090942\n",
      "EPOCH:34 Step:16875 Val_loss:1.2926400899887085\n",
      "EPOCH:34 Step:16900 Val_loss:1.2899357080459595\n",
      "EPOCH:34 Step:16925 Val_loss:1.2880635261535645\n",
      "EPOCH:34 Step:16950 Val_loss:1.2932356595993042\n",
      "EPOCH:34 Step:16975 Val_loss:1.288703441619873\n",
      "EPOCH:34 Step:17000 Val_loss:1.2862375974655151\n",
      "EPOCH:34 Step:17025 Val_loss:1.2939045429229736\n",
      "EPOCH:34 Step:17050 Val_loss:1.2937554121017456\n",
      "EPOCH:34 Step:17075 Val_loss:1.2936320304870605\n",
      "EPOCH:34 Step:17100 Val_loss:1.2952805757522583\n",
      "EPOCH:34 Step:17125 Val_loss:1.297526240348816\n",
      "EPOCH:34 Step:17150 Val_loss:1.2952531576156616\n",
      "EPOCH:35 Step:17175 Val_loss:1.2922275066375732\n",
      "EPOCH:35 Step:17200 Val_loss:1.3032335042953491\n",
      "EPOCH:35 Step:17225 Val_loss:1.2850512266159058\n",
      "EPOCH:35 Step:17250 Val_loss:1.2870745658874512\n",
      "EPOCH:35 Step:17275 Val_loss:1.282105803489685\n",
      "EPOCH:35 Step:17300 Val_loss:1.2852801084518433\n",
      "EPOCH:35 Step:17325 Val_loss:1.289021372795105\n",
      "EPOCH:35 Step:17350 Val_loss:1.2939850091934204\n",
      "EPOCH:35 Step:17375 Val_loss:1.287830114364624\n",
      "EPOCH:35 Step:17400 Val_loss:1.2836655378341675\n",
      "EPOCH:35 Step:17425 Val_loss:1.286370038986206\n",
      "EPOCH:35 Step:17450 Val_loss:1.2898433208465576\n",
      "EPOCH:35 Step:17475 Val_loss:1.2826812267303467\n",
      "EPOCH:35 Step:17500 Val_loss:1.290237545967102\n",
      "EPOCH:35 Step:17525 Val_loss:1.2884854078292847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:35 Step:17550 Val_loss:1.2888833284378052\n",
      "EPOCH:35 Step:17575 Val_loss:1.2908598184585571\n",
      "EPOCH:35 Step:17600 Val_loss:1.292309045791626\n",
      "EPOCH:35 Step:17625 Val_loss:1.2937982082366943\n",
      "EPOCH:36 Step:17650 Val_loss:1.2917218208312988\n",
      "EPOCH:36 Step:17675 Val_loss:1.2930468320846558\n",
      "EPOCH:36 Step:17700 Val_loss:1.294142723083496\n",
      "EPOCH:36 Step:17725 Val_loss:1.2937411069869995\n",
      "EPOCH:36 Step:17750 Val_loss:1.2893911600112915\n",
      "EPOCH:36 Step:17775 Val_loss:1.2838099002838135\n",
      "EPOCH:36 Step:17800 Val_loss:1.2895528078079224\n",
      "EPOCH:36 Step:17825 Val_loss:1.293517827987671\n",
      "EPOCH:36 Step:17850 Val_loss:1.2902915477752686\n",
      "EPOCH:36 Step:17875 Val_loss:1.2898502349853516\n",
      "EPOCH:36 Step:17900 Val_loss:1.283109426498413\n",
      "EPOCH:36 Step:17925 Val_loss:1.289252758026123\n",
      "EPOCH:36 Step:17950 Val_loss:1.290122389793396\n",
      "EPOCH:36 Step:17975 Val_loss:1.2897348403930664\n",
      "EPOCH:36 Step:18000 Val_loss:1.2920572757720947\n",
      "EPOCH:36 Step:18025 Val_loss:1.2929400205612183\n",
      "EPOCH:36 Step:18050 Val_loss:1.2942478656768799\n",
      "EPOCH:36 Step:18075 Val_loss:1.2931140661239624\n",
      "EPOCH:36 Step:18100 Val_loss:1.295362114906311\n",
      "EPOCH:36 Step:18125 Val_loss:1.2908326387405396\n",
      "EPOCH:37 Step:18150 Val_loss:1.2896649837493896\n",
      "EPOCH:37 Step:18175 Val_loss:1.2904024124145508\n",
      "EPOCH:37 Step:18200 Val_loss:1.2930432558059692\n",
      "EPOCH:37 Step:18225 Val_loss:1.290074110031128\n",
      "EPOCH:37 Step:18250 Val_loss:1.2982914447784424\n",
      "EPOCH:37 Step:18275 Val_loss:1.2933753728866577\n",
      "EPOCH:37 Step:18300 Val_loss:1.2978023290634155\n",
      "EPOCH:37 Step:18325 Val_loss:1.2956353425979614\n",
      "EPOCH:37 Step:18350 Val_loss:1.2931996583938599\n",
      "EPOCH:37 Step:18375 Val_loss:1.290405511856079\n",
      "EPOCH:37 Step:18400 Val_loss:1.2897331714630127\n",
      "EPOCH:37 Step:18425 Val_loss:1.2905365228652954\n",
      "EPOCH:37 Step:18450 Val_loss:1.2874391078948975\n",
      "EPOCH:37 Step:18475 Val_loss:1.290318489074707\n",
      "EPOCH:37 Step:18500 Val_loss:1.293991208076477\n",
      "EPOCH:37 Step:18525 Val_loss:1.2990281581878662\n",
      "EPOCH:37 Step:18550 Val_loss:1.2962092161178589\n",
      "EPOCH:37 Step:18575 Val_loss:1.3010916709899902\n",
      "EPOCH:37 Step:18600 Val_loss:1.297078013420105\n",
      "EPOCH:38 Step:18625 Val_loss:1.295856237411499\n",
      "EPOCH:38 Step:18650 Val_loss:1.2963390350341797\n",
      "EPOCH:38 Step:18675 Val_loss:1.2950912714004517\n",
      "EPOCH:38 Step:18700 Val_loss:1.2937082052230835\n",
      "EPOCH:38 Step:18725 Val_loss:1.291583776473999\n",
      "EPOCH:38 Step:18750 Val_loss:1.2886251211166382\n",
      "EPOCH:38 Step:18775 Val_loss:1.2899069786071777\n",
      "EPOCH:38 Step:18800 Val_loss:1.2903223037719727\n",
      "EPOCH:38 Step:18825 Val_loss:1.2968708276748657\n",
      "EPOCH:38 Step:18850 Val_loss:1.2927857637405396\n",
      "EPOCH:38 Step:18875 Val_loss:1.285258412361145\n",
      "EPOCH:38 Step:18900 Val_loss:1.2921626567840576\n",
      "EPOCH:38 Step:18925 Val_loss:1.2957820892333984\n",
      "EPOCH:38 Step:18950 Val_loss:1.2904845476150513\n",
      "EPOCH:38 Step:18975 Val_loss:1.2890714406967163\n",
      "EPOCH:38 Step:19000 Val_loss:1.2914707660675049\n",
      "EPOCH:38 Step:19025 Val_loss:1.2857112884521484\n",
      "EPOCH:38 Step:19050 Val_loss:1.2863495349884033\n",
      "EPOCH:38 Step:19075 Val_loss:1.2885884046554565\n",
      "EPOCH:38 Step:19100 Val_loss:1.2931640148162842\n",
      "EPOCH:39 Step:19125 Val_loss:1.292227864265442\n",
      "EPOCH:39 Step:19150 Val_loss:1.2934951782226562\n",
      "EPOCH:39 Step:19175 Val_loss:1.2990636825561523\n",
      "EPOCH:39 Step:19200 Val_loss:1.293501377105713\n",
      "EPOCH:39 Step:19225 Val_loss:1.2928391695022583\n",
      "EPOCH:39 Step:19250 Val_loss:1.292117714881897\n",
      "EPOCH:39 Step:19275 Val_loss:1.296700358390808\n",
      "EPOCH:39 Step:19300 Val_loss:1.2963356971740723\n",
      "EPOCH:39 Step:19325 Val_loss:1.2940000295639038\n",
      "EPOCH:39 Step:19350 Val_loss:1.2961653470993042\n",
      "EPOCH:39 Step:19375 Val_loss:1.2917526960372925\n",
      "EPOCH:39 Step:19400 Val_loss:1.2989320755004883\n",
      "EPOCH:39 Step:19425 Val_loss:1.2964739799499512\n",
      "EPOCH:39 Step:19450 Val_loss:1.2951455116271973\n",
      "EPOCH:39 Step:19475 Val_loss:1.2967844009399414\n",
      "EPOCH:39 Step:19500 Val_loss:1.2987359762191772\n",
      "EPOCH:39 Step:19525 Val_loss:1.2968980073928833\n",
      "EPOCH:39 Step:19550 Val_loss:1.2986366748809814\n",
      "EPOCH:39 Step:19575 Val_loss:1.2977466583251953\n",
      "EPOCH:39 Step:19600 Val_loss:1.2975045442581177\n",
      "Training time was -246.63160417079925 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "epochs= 40\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "\n",
    "tracker = 0\n",
    "num_chars = max(encoder_text)+1\n",
    "\n",
    "model.train()\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    for x, y in generate_batches(train_data, batch_size, seq_len):\n",
    "        \n",
    "        tracker +=1\n",
    "        x = one_hot_encoder(x, num_chars) #Before:(100, 100), After:(100, 100 , 84)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_out, hidden = model.forward(inputs, hidden) #lstm_out : torch.Size([10000, 84])\n",
    "        loss = criterion(lstm_out, targets.view(batch_size*seq_len).long()) #target: (10000)\n",
    "        loss.backward()\n",
    "        \n",
    "        #Getting rid of possible exploding gradient\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Check on validation\n",
    "        if tracker %25==0:\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x, y in generate_batches(test_data, batch_size, seq_len):\n",
    "                x = one_hot_encoder(x, num_chars)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                lstm_out, val_hidden = model.forward(inputs, val_hidden)\n",
    "                val_loss = criterion(lstm_out, targets.view(batch_size*seq_len).long())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            #Trainnin again\n",
    "            model.train()\n",
    "            print(f'EPOCH:{i} Step:{tracker} Val_loss:{val_loss.item()}')\n",
    "\n",
    "final_time = start_time - time.time()\n",
    "print(f'Training time was {final_time/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daade85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder(x,84).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a5b212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 10)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 10), torch.randn(1, 1, 10))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b4033bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bdf60599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3085d7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b25bc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hidden512_layers3_shakes.net'\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c99085c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder['K']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4555eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[76]]), (1, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[model.encoder['K']]]), np.array([[model.encoder['K']]]).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "960d98a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0.]]], dtype=float32),\n",
       " (1, 1, 84))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(np.array([[model.encoder['K']]]), len(model.all_chars)), one_hot_encoder(np.array([[model.encoder['K']]]), len(model.all_chars)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "40ebc26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(one_hot_encoder(np.array([[model.encoder['K']]]), len(model.all_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3297996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_state(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dbeefb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([state.data for state in model.hidden_state(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "141c33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, outs = model(torch.from_numpy(one_hot_encoder(np.array([[model.encoder['K']]]), len(model.all_chars))).cuda(),\n",
    "                  model.hidden_state(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8c1408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8353, -1.7359,  2.5463, -2.7098, -3.3728,  0.9534, -2.1307, -2.6921,\n",
       "          -0.3939,  1.1469,  0.5161, -1.4732, -1.2952,  1.9920, -0.1059, -0.8318,\n",
       "          -7.6345, -2.0398, -2.6767, -0.7123, -4.6380, -2.3536, -3.7381, -2.3122,\n",
       "          -1.6228,  2.0972, -2.5732,  2.3281,  2.8110, -4.2836,  0.1369,  0.9350,\n",
       "           1.1316, -2.5912, -0.7516, -1.4473,  2.7744, -1.9449, -0.1226,  2.5396,\n",
       "          -4.4774,  0.5985,  0.5502, -3.0845,  0.2442,  0.4286,  0.6152, -0.9765,\n",
       "          -3.2988, -2.1381, -1.2280, -1.1823, -0.8534, -0.8591, -0.4164, -0.5458,\n",
       "          -4.3499, -1.9504,  1.7265, -3.6936,  1.9697, -0.1959, -1.8800,  0.0487,\n",
       "          -3.3206,  1.0551, -1.1534, -0.9623,  0.0830,  0.1731, -1.4498, -5.3504,\n",
       "           1.4350, -0.5176, -2.6399, -0.8766, -1.4779, -0.4831,  1.0546, -2.4944,\n",
       "          -4.8549, -4.8643, -3.5477, -1.7324]], device='cuda:0',\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " torch.Size([1, 84]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp, inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b7c7855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.1122]], device='cuda:0', grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[28]], device='cuda:0'))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(inp, dim=1).topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3d561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1524e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5b9c998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "    encoded_text = model.encoder[char]\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "    inputs = torch.from_numpy(encoded_text)\n",
    "    if model.use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "    hidden = tuple([state.data for state in hidden])\n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "    probs = F.softmax(lstm_out, dim=1).data\n",
    "    \n",
    "    if model.use_gpu:\n",
    "        probs = probs.cpu()\n",
    "    \n",
    "    probs, index_pos = probs.topk(k)\n",
    "    index_pos = index_pos.numpy().squeeze()\n",
    "    probs = probs.numpy().flatten()\n",
    "    probs = probs/probs.sum()\n",
    "    char = np.random.choice(index_pos, p=probs)\n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3c5b6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "    if model.use_gpu:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "        \n",
    "    model.eval()\n",
    "    output_chars = [c for c in seed]\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    for char in seed:\n",
    "        char,hidden = predict_next_char(model,  char, hidden, k)\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    for i in range(size):\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k)\n",
    "        output_chars.append(char)\n",
    "    return ''.join(output_chars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bafd64a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheR BANDITTIE and ANTONY\n",
      "\n",
      "Enter CAESAR, and others, attended with the tongues of the country,\n",
      "                                    the stocks of her sound  \n",
      "\n",
      "                         Enter CLEOPATRA with a part of the confidence\n",
      "\n",
      "  PROSPERO. A soul on high and soldier, and the stars,\n",
      "    A soul to this that there were beard a wars,\n",
      "    And therefore, and a wanton watch of state\n",
      "    Than the time so still would be so to thee.\n",
      "    Therefore I will, and the success of his\n",
      "    That should be that to them and so thou art\n",
      "    Without thy branches.\n",
      "  CLEOPATRA. Why, here it well.\n",
      "                       Exeunt CONSTABLE, and SIR HUGH EVANS\n",
      "    The streets, my liege! What shall I see thy hand,\n",
      "    And there is the companion of the world,\n",
      "    The cares of that the commons that the face\n",
      "    Will show his foreshe seek to speak? This strength\n",
      "    Which should have then the state of this third body,\n",
      "    That what he should be so so sensel's sound,\n",
      "    Till he did see a shoulder of this son,\n",
      "    They see\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddc01b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cudatest",
   "language": "python",
   "name": "cudatest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
